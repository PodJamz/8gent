{
  "version": 3,
  "sources": [],
  "sections": [
    {"offset": {"line": 40, "column": 0}, "map": {"version":3,"sources":["file:///Users/jamesspalding/OpenClaw-OS/src/lib/ollama/client.ts"],"sourcesContent":["/**\n * Ollama Client - Phase 3 Local LLM Integration\n *\n * Client library for interacting with local Ollama server.\n * Supports chat, generate, model listing, and health checks.\n */\n\n// ============================================================================\n// Types\n// ============================================================================\n\nexport interface OllamaConfig {\n  baseUrl: string;\n  timeout?: number; // ms\n  defaultModel?: string;\n}\n\nexport interface OllamaModel {\n  name: string;\n  modified_at: string;\n  size: number;\n  digest: string;\n  details?: {\n    format: string;\n    family: string;\n    parameter_size: string;\n    quantization_level: string;\n  };\n}\n\nexport interface OllamaMessage {\n  role: 'system' | 'user' | 'assistant';\n  content: string;\n}\n\nexport interface OllamaChatRequest {\n  model: string;\n  messages: OllamaMessage[];\n  stream?: boolean;\n  options?: {\n    temperature?: number;\n    top_p?: number;\n    top_k?: number;\n    num_predict?: number;\n    stop?: string[];\n  };\n}\n\nexport interface OllamaChatResponse {\n  model: string;\n  created_at: string;\n  message: OllamaMessage;\n  done: boolean;\n  total_duration?: number;\n  load_duration?: number;\n  prompt_eval_count?: number;\n  eval_count?: number;\n  eval_duration?: number;\n}\n\nexport interface OllamaGenerateRequest {\n  model: string;\n  prompt: string;\n  stream?: boolean;\n  system?: string;\n  options?: {\n    temperature?: number;\n    top_p?: number;\n    top_k?: number;\n    num_predict?: number;\n    stop?: string[];\n  };\n}\n\nexport interface OllamaGenerateResponse {\n  model: string;\n  created_at: string;\n  response: string;\n  done: boolean;\n  context?: number[];\n  total_duration?: number;\n  load_duration?: number;\n  prompt_eval_count?: number;\n  eval_count?: number;\n  eval_duration?: number;\n}\n\nexport interface OllamaHealthStatus {\n  connected: boolean;\n  version?: string;\n  models: OllamaModel[];\n  error?: string;\n  latencyMs?: number;\n}\n\n// ============================================================================\n// Client Class\n// ============================================================================\n\nexport class OllamaClient {\n  private baseUrl: string;\n  private timeout: number;\n  private defaultModel: string;\n\n  constructor(config: OllamaConfig) {\n    this.baseUrl = config.baseUrl.replace(/\\/$/, ''); // Remove trailing slash\n    this.timeout = config.timeout ?? 30000;\n    this.defaultModel = config.defaultModel ?? 'gpt-oss:20b';\n  }\n\n  /**\n   * Check if Ollama server is reachable and get available models\n   */\n  async health(): Promise<OllamaHealthStatus> {\n    const start = Date.now();\n\n    try {\n      // Try to list models (this also verifies server is running)\n      const response = await this.fetchWithTimeout(`${this.baseUrl}/api/tags`, {\n        method: 'GET',\n      });\n\n      if (!response.ok) {\n        return {\n          connected: false,\n          models: [],\n          error: `Server returned ${response.status}`,\n          latencyMs: Date.now() - start,\n        };\n      }\n\n      const data = await response.json();\n      const models: OllamaModel[] = data.models || [];\n\n      // Also try to get version\n      let version: string | undefined;\n      try {\n        const versionResponse = await this.fetchWithTimeout(`${this.baseUrl}/api/version`, {\n          method: 'GET',\n        });\n        if (versionResponse.ok) {\n          const versionData = await versionResponse.json();\n          version = versionData.version;\n        }\n      } catch (error) {\n        // Version endpoint might not exist in older versions - log at debug level\n        console.debug('[Ollama] Version endpoint not available:', error);\n      }\n\n      return {\n        connected: true,\n        version,\n        models,\n        latencyMs: Date.now() - start,\n      };\n    } catch (error) {\n      return {\n        connected: false,\n        models: [],\n        error: error instanceof Error ? error.message : 'Connection failed',\n        latencyMs: Date.now() - start,\n      };\n    }\n  }\n\n  /**\n   * List available models\n   */\n  async listModels(): Promise<OllamaModel[]> {\n    const response = await this.fetchWithTimeout(`${this.baseUrl}/api/tags`, {\n      method: 'GET',\n    });\n\n    if (!response.ok) {\n      throw new Error(`Failed to list models: ${response.status}`);\n    }\n\n    const data = await response.json();\n    return data.models || [];\n  }\n\n  /**\n   * Chat with a model (non-streaming)\n   */\n  async chat(request: OllamaChatRequest): Promise<OllamaChatResponse> {\n    const response = await this.fetchWithTimeout(`${this.baseUrl}/api/chat`, {\n      method: 'POST',\n      headers: { 'Content-Type': 'application/json' },\n      body: JSON.stringify({\n        ...request,\n        model: request.model || this.defaultModel,\n        stream: false,\n      }),\n    });\n\n    if (!response.ok) {\n      const error = await response.text();\n      throw new Error(`Chat failed: ${response.status} - ${error}`);\n    }\n\n    return response.json();\n  }\n\n  /**\n   * Chat with a model (streaming)\n   * Returns an async generator that yields partial responses\n   */\n  async *chatStream(request: OllamaChatRequest): AsyncGenerator<OllamaChatResponse> {\n    const response = await this.fetchWithTimeout(`${this.baseUrl}/api/chat`, {\n      method: 'POST',\n      headers: { 'Content-Type': 'application/json' },\n      body: JSON.stringify({\n        ...request,\n        model: request.model || this.defaultModel,\n        stream: true,\n      }),\n    });\n\n    if (!response.ok) {\n      const error = await response.text();\n      throw new Error(`Chat stream failed: ${response.status} - ${error}`);\n    }\n\n    const reader = response.body?.getReader();\n    if (!reader) {\n      throw new Error('No response body');\n    }\n\n    const decoder = new TextDecoder();\n    let buffer = '';\n\n    try {\n      while (true) {\n        const { done, value } = await reader.read();\n\n        if (done) break;\n\n        buffer += decoder.decode(value, { stream: true });\n        const lines = buffer.split('\\n');\n        buffer = lines.pop() || '';\n\n        for (const line of lines) {\n          if (line.trim()) {\n            try {\n              const data = JSON.parse(line) as OllamaChatResponse;\n              yield data;\n            } catch (error) {\n              // Log malformed JSON for debugging but continue processing\n              console.error('[Ollama] Malformed JSON in chat stream:', line.substring(0, 100), error);\n            }\n          }\n        }\n      }\n\n      // Process remaining buffer\n      if (buffer.trim()) {\n        try {\n          const data = JSON.parse(buffer) as OllamaChatResponse;\n          yield data;\n        } catch (error) {\n          // Log malformed JSON for debugging\n          console.error('[Ollama] Malformed JSON in chat buffer:', buffer.substring(0, 100), error);\n        }\n      }\n    } finally {\n      reader.releaseLock();\n    }\n  }\n\n  /**\n   * Generate text (non-streaming)\n   */\n  async generate(request: OllamaGenerateRequest): Promise<OllamaGenerateResponse> {\n    const response = await this.fetchWithTimeout(`${this.baseUrl}/api/generate`, {\n      method: 'POST',\n      headers: { 'Content-Type': 'application/json' },\n      body: JSON.stringify({\n        ...request,\n        model: request.model || this.defaultModel,\n        stream: false,\n      }),\n    });\n\n    if (!response.ok) {\n      const error = await response.text();\n      throw new Error(`Generate failed: ${response.status} - ${error}`);\n    }\n\n    return response.json();\n  }\n\n  /**\n   * Generate text (streaming)\n   */\n  async *generateStream(request: OllamaGenerateRequest): AsyncGenerator<OllamaGenerateResponse> {\n    const response = await this.fetchWithTimeout(`${this.baseUrl}/api/generate`, {\n      method: 'POST',\n      headers: { 'Content-Type': 'application/json' },\n      body: JSON.stringify({\n        ...request,\n        model: request.model || this.defaultModel,\n        stream: true,\n      }),\n    });\n\n    if (!response.ok) {\n      const error = await response.text();\n      throw new Error(`Generate stream failed: ${response.status} - ${error}`);\n    }\n\n    const reader = response.body?.getReader();\n    if (!reader) {\n      throw new Error('No response body');\n    }\n\n    const decoder = new TextDecoder();\n    let buffer = '';\n\n    try {\n      while (true) {\n        const { done, value } = await reader.read();\n\n        if (done) break;\n\n        buffer += decoder.decode(value, { stream: true });\n        const lines = buffer.split('\\n');\n        buffer = lines.pop() || '';\n\n        for (const line of lines) {\n          if (line.trim()) {\n            try {\n              const data = JSON.parse(line) as OllamaGenerateResponse;\n              yield data;\n            } catch (error) {\n              // Log malformed JSON for debugging but continue processing\n              console.error('[Ollama] Malformed JSON in generate stream:', line.substring(0, 100), error);\n            }\n          }\n        }\n      }\n\n      // Process remaining buffer\n      if (buffer.trim()) {\n        try {\n          const data = JSON.parse(buffer) as OllamaGenerateResponse;\n          yield data;\n        } catch (error) {\n          // Log malformed JSON for debugging\n          console.error('[Ollama] Malformed JSON in generate buffer:', buffer.substring(0, 100), error);\n        }\n      }\n    } finally {\n      reader.releaseLock();\n    }\n  }\n\n  /**\n   * Pull a model from Ollama registry\n   */\n  async pullModel(modelName: string): Promise<void> {\n    const response = await fetch(`${this.baseUrl}/api/pull`, {\n      method: 'POST',\n      headers: { 'Content-Type': 'application/json' },\n      body: JSON.stringify({ name: modelName }),\n    });\n\n    if (!response.ok) {\n      const error = await response.text();\n      throw new Error(`Failed to pull model: ${response.status} - ${error}`);\n    }\n\n    // Wait for the model to be pulled (this can take a while)\n    const reader = response.body?.getReader();\n    if (reader) {\n      const decoder = new TextDecoder();\n      while (true) {\n        const { done } = await reader.read();\n        if (done) break;\n      }\n      reader.releaseLock();\n    }\n  }\n\n  /**\n   * Fetch with timeout\n   */\n  private async fetchWithTimeout(url: string, options: RequestInit): Promise<Response> {\n    const controller = new AbortController();\n    const timeoutId = setTimeout(() => controller.abort(), this.timeout);\n\n    try {\n      const response = await fetch(url, {\n        ...options,\n        signal: controller.signal,\n      });\n      return response;\n    } catch (error) {\n      if (error instanceof Error && error.name === 'AbortError') {\n        throw new Error(`Request timed out after ${this.timeout}ms`);\n      }\n      throw error;\n    } finally {\n      clearTimeout(timeoutId);\n    }\n  }\n}\n\n// ============================================================================\n// Singleton Instance\n// ============================================================================\n\nlet defaultClient: OllamaClient | null = null;\n\n/**\n * Get the default Ollama client\n * Uses OLLAMA_BASE_URL env var or defaults to localhost:11434\n */\nexport function getOllamaClient(config?: Partial<OllamaConfig>): OllamaClient {\n  if (!defaultClient || config) {\n    defaultClient = new OllamaClient({\n      baseUrl: config?.baseUrl ?? process.env.OLLAMA_BASE_URL ?? 'http://localhost:11434',\n      timeout: config?.timeout ?? 30000,\n      defaultModel: config?.defaultModel ?? process.env.OLLAMA_DEFAULT_MODEL ?? 'gpt-oss:20b',\n    });\n  }\n  return defaultClient;\n}\n\n/**\n * Quick health check for Ollama\n */\nexport async function checkOllamaHealth(baseUrl?: string): Promise<OllamaHealthStatus> {\n  const client = new OllamaClient({\n    baseUrl: baseUrl ?? process.env.OLLAMA_BASE_URL ?? 'http://localhost:11434',\n    timeout: 5000, // Quick health check\n  });\n  return client.health();\n}\n"],"names":[],"mappings":"AAAA;;;;;CAKC,GAED,+EAA+E;AAC/E,QAAQ;AACR,+EAA+E;;;;;;;;;AA0FxE,MAAM;IACH,QAAgB;IAChB,QAAgB;IAChB,aAAqB;IAE7B,YAAY,MAAoB,CAAE;QAChC,IAAI,CAAC,OAAO,GAAG,OAAO,OAAO,CAAC,OAAO,CAAC,OAAO,KAAK,wBAAwB;QAC1E,IAAI,CAAC,OAAO,GAAG,OAAO,OAAO,IAAI;QACjC,IAAI,CAAC,YAAY,GAAG,OAAO,YAAY,IAAI;IAC7C;IAEA;;GAEC,GACD,MAAM,SAAsC;QAC1C,MAAM,QAAQ,KAAK,GAAG;QAEtB,IAAI;YACF,4DAA4D;YAC5D,MAAM,WAAW,MAAM,IAAI,CAAC,gBAAgB,CAAC,GAAG,IAAI,CAAC,OAAO,CAAC,SAAS,CAAC,EAAE;gBACvE,QAAQ;YACV;YAEA,IAAI,CAAC,SAAS,EAAE,EAAE;gBAChB,OAAO;oBACL,WAAW;oBACX,QAAQ,EAAE;oBACV,OAAO,CAAC,gBAAgB,EAAE,SAAS,MAAM,EAAE;oBAC3C,WAAW,KAAK,GAAG,KAAK;gBAC1B;YACF;YAEA,MAAM,OAAO,MAAM,SAAS,IAAI;YAChC,MAAM,SAAwB,KAAK,MAAM,IAAI,EAAE;YAE/C,0BAA0B;YAC1B,IAAI;YACJ,IAAI;gBACF,MAAM,kBAAkB,MAAM,IAAI,CAAC,gBAAgB,CAAC,GAAG,IAAI,CAAC,OAAO,CAAC,YAAY,CAAC,EAAE;oBACjF,QAAQ;gBACV;gBACA,IAAI,gBAAgB,EAAE,EAAE;oBACtB,MAAM,cAAc,MAAM,gBAAgB,IAAI;oBAC9C,UAAU,YAAY,OAAO;gBAC/B;YACF,EAAE,OAAO,OAAO;gBACd,0EAA0E;gBAC1E,QAAQ,KAAK,CAAC,4CAA4C;YAC5D;YAEA,OAAO;gBACL,WAAW;gBACX;gBACA;gBACA,WAAW,KAAK,GAAG,KAAK;YAC1B;QACF,EAAE,OAAO,OAAO;YACd,OAAO;gBACL,WAAW;gBACX,QAAQ,EAAE;gBACV,OAAO,iBAAiB,QAAQ,MAAM,OAAO,GAAG;gBAChD,WAAW,KAAK,GAAG,KAAK;YAC1B;QACF;IACF;IAEA;;GAEC,GACD,MAAM,aAAqC;QACzC,MAAM,WAAW,MAAM,IAAI,CAAC,gBAAgB,CAAC,GAAG,IAAI,CAAC,OAAO,CAAC,SAAS,CAAC,EAAE;YACvE,QAAQ;QACV;QAEA,IAAI,CAAC,SAAS,EAAE,EAAE;YAChB,MAAM,IAAI,MAAM,CAAC,uBAAuB,EAAE,SAAS,MAAM,EAAE;QAC7D;QAEA,MAAM,OAAO,MAAM,SAAS,IAAI;QAChC,OAAO,KAAK,MAAM,IAAI,EAAE;IAC1B;IAEA;;GAEC,GACD,MAAM,KAAK,OAA0B,EAA+B;QAClE,MAAM,WAAW,MAAM,IAAI,CAAC,gBAAgB,CAAC,GAAG,IAAI,CAAC,OAAO,CAAC,SAAS,CAAC,EAAE;YACvE,QAAQ;YACR,SAAS;gBAAE,gBAAgB;YAAmB;YAC9C,MAAM,KAAK,SAAS,CAAC;gBACnB,GAAG,OAAO;gBACV,OAAO,QAAQ,KAAK,IAAI,IAAI,CAAC,YAAY;gBACzC,QAAQ;YACV;QACF;QAEA,IAAI,CAAC,SAAS,EAAE,EAAE;YAChB,MAAM,QAAQ,MAAM,SAAS,IAAI;YACjC,MAAM,IAAI,MAAM,CAAC,aAAa,EAAE,SAAS,MAAM,CAAC,GAAG,EAAE,OAAO;QAC9D;QAEA,OAAO,SAAS,IAAI;IACtB;IAEA;;;GAGC,GACD,OAAO,WAAW,OAA0B,EAAsC;QAChF,MAAM,WAAW,MAAM,IAAI,CAAC,gBAAgB,CAAC,GAAG,IAAI,CAAC,OAAO,CAAC,SAAS,CAAC,EAAE;YACvE,QAAQ;YACR,SAAS;gBAAE,gBAAgB;YAAmB;YAC9C,MAAM,KAAK,SAAS,CAAC;gBACnB,GAAG,OAAO;gBACV,OAAO,QAAQ,KAAK,IAAI,IAAI,CAAC,YAAY;gBACzC,QAAQ;YACV;QACF;QAEA,IAAI,CAAC,SAAS,EAAE,EAAE;YAChB,MAAM,QAAQ,MAAM,SAAS,IAAI;YACjC,MAAM,IAAI,MAAM,CAAC,oBAAoB,EAAE,SAAS,MAAM,CAAC,GAAG,EAAE,OAAO;QACrE;QAEA,MAAM,SAAS,SAAS,IAAI,EAAE;QAC9B,IAAI,CAAC,QAAQ;YACX,MAAM,IAAI,MAAM;QAClB;QAEA,MAAM,UAAU,IAAI;QACpB,IAAI,SAAS;QAEb,IAAI;YACF,MAAO,KAAM;gBACX,MAAM,EAAE,IAAI,EAAE,KAAK,EAAE,GAAG,MAAM,OAAO,IAAI;gBAEzC,IAAI,MAAM;gBAEV,UAAU,QAAQ,MAAM,CAAC,OAAO;oBAAE,QAAQ;gBAAK;gBAC/C,MAAM,QAAQ,OAAO,KAAK,CAAC;gBAC3B,SAAS,MAAM,GAAG,MAAM;gBAExB,KAAK,MAAM,QAAQ,MAAO;oBACxB,IAAI,KAAK,IAAI,IAAI;wBACf,IAAI;4BACF,MAAM,OAAO,KAAK,KAAK,CAAC;4BACxB,MAAM;wBACR,EAAE,OAAO,OAAO;4BACd,2DAA2D;4BAC3D,QAAQ,KAAK,CAAC,2CAA2C,KAAK,SAAS,CAAC,GAAG,MAAM;wBACnF;oBACF;gBACF;YACF;YAEA,2BAA2B;YAC3B,IAAI,OAAO,IAAI,IAAI;gBACjB,IAAI;oBACF,MAAM,OAAO,KAAK,KAAK,CAAC;oBACxB,MAAM;gBACR,EAAE,OAAO,OAAO;oBACd,mCAAmC;oBACnC,QAAQ,KAAK,CAAC,2CAA2C,OAAO,SAAS,CAAC,GAAG,MAAM;gBACrF;YACF;QACF,SAAU;YACR,OAAO,WAAW;QACpB;IACF;IAEA;;GAEC,GACD,MAAM,SAAS,OAA8B,EAAmC;QAC9E,MAAM,WAAW,MAAM,IAAI,CAAC,gBAAgB,CAAC,GAAG,IAAI,CAAC,OAAO,CAAC,aAAa,CAAC,EAAE;YAC3E,QAAQ;YACR,SAAS;gBAAE,gBAAgB;YAAmB;YAC9C,MAAM,KAAK,SAAS,CAAC;gBACnB,GAAG,OAAO;gBACV,OAAO,QAAQ,KAAK,IAAI,IAAI,CAAC,YAAY;gBACzC,QAAQ;YACV;QACF;QAEA,IAAI,CAAC,SAAS,EAAE,EAAE;YAChB,MAAM,QAAQ,MAAM,SAAS,IAAI;YACjC,MAAM,IAAI,MAAM,CAAC,iBAAiB,EAAE,SAAS,MAAM,CAAC,GAAG,EAAE,OAAO;QAClE;QAEA,OAAO,SAAS,IAAI;IACtB;IAEA;;GAEC,GACD,OAAO,eAAe,OAA8B,EAA0C;QAC5F,MAAM,WAAW,MAAM,IAAI,CAAC,gBAAgB,CAAC,GAAG,IAAI,CAAC,OAAO,CAAC,aAAa,CAAC,EAAE;YAC3E,QAAQ;YACR,SAAS;gBAAE,gBAAgB;YAAmB;YAC9C,MAAM,KAAK,SAAS,CAAC;gBACnB,GAAG,OAAO;gBACV,OAAO,QAAQ,KAAK,IAAI,IAAI,CAAC,YAAY;gBACzC,QAAQ;YACV;QACF;QAEA,IAAI,CAAC,SAAS,EAAE,EAAE;YAChB,MAAM,QAAQ,MAAM,SAAS,IAAI;YACjC,MAAM,IAAI,MAAM,CAAC,wBAAwB,EAAE,SAAS,MAAM,CAAC,GAAG,EAAE,OAAO;QACzE;QAEA,MAAM,SAAS,SAAS,IAAI,EAAE;QAC9B,IAAI,CAAC,QAAQ;YACX,MAAM,IAAI,MAAM;QAClB;QAEA,MAAM,UAAU,IAAI;QACpB,IAAI,SAAS;QAEb,IAAI;YACF,MAAO,KAAM;gBACX,MAAM,EAAE,IAAI,EAAE,KAAK,EAAE,GAAG,MAAM,OAAO,IAAI;gBAEzC,IAAI,MAAM;gBAEV,UAAU,QAAQ,MAAM,CAAC,OAAO;oBAAE,QAAQ;gBAAK;gBAC/C,MAAM,QAAQ,OAAO,KAAK,CAAC;gBAC3B,SAAS,MAAM,GAAG,MAAM;gBAExB,KAAK,MAAM,QAAQ,MAAO;oBACxB,IAAI,KAAK,IAAI,IAAI;wBACf,IAAI;4BACF,MAAM,OAAO,KAAK,KAAK,CAAC;4BACxB,MAAM;wBACR,EAAE,OAAO,OAAO;4BACd,2DAA2D;4BAC3D,QAAQ,KAAK,CAAC,+CAA+C,KAAK,SAAS,CAAC,GAAG,MAAM;wBACvF;oBACF;gBACF;YACF;YAEA,2BAA2B;YAC3B,IAAI,OAAO,IAAI,IAAI;gBACjB,IAAI;oBACF,MAAM,OAAO,KAAK,KAAK,CAAC;oBACxB,MAAM;gBACR,EAAE,OAAO,OAAO;oBACd,mCAAmC;oBACnC,QAAQ,KAAK,CAAC,+CAA+C,OAAO,SAAS,CAAC,GAAG,MAAM;gBACzF;YACF;QACF,SAAU;YACR,OAAO,WAAW;QACpB;IACF;IAEA;;GAEC,GACD,MAAM,UAAU,SAAiB,EAAiB;QAChD,MAAM,WAAW,MAAM,MAAM,GAAG,IAAI,CAAC,OAAO,CAAC,SAAS,CAAC,EAAE;YACvD,QAAQ;YACR,SAAS;gBAAE,gBAAgB;YAAmB;YAC9C,MAAM,KAAK,SAAS,CAAC;gBAAE,MAAM;YAAU;QACzC;QAEA,IAAI,CAAC,SAAS,EAAE,EAAE;YAChB,MAAM,QAAQ,MAAM,SAAS,IAAI;YACjC,MAAM,IAAI,MAAM,CAAC,sBAAsB,EAAE,SAAS,MAAM,CAAC,GAAG,EAAE,OAAO;QACvE;QAEA,0DAA0D;QAC1D,MAAM,SAAS,SAAS,IAAI,EAAE;QAC9B,IAAI,QAAQ;YACV,MAAM,UAAU,IAAI;YACpB,MAAO,KAAM;gBACX,MAAM,EAAE,IAAI,EAAE,GAAG,MAAM,OAAO,IAAI;gBAClC,IAAI,MAAM;YACZ;YACA,OAAO,WAAW;QACpB;IACF;IAEA;;GAEC,GACD,MAAc,iBAAiB,GAAW,EAAE,OAAoB,EAAqB;QACnF,MAAM,aAAa,IAAI;QACvB,MAAM,YAAY,WAAW,IAAM,WAAW,KAAK,IAAI,IAAI,CAAC,OAAO;QAEnE,IAAI;YACF,MAAM,WAAW,MAAM,MAAM,KAAK;gBAChC,GAAG,OAAO;gBACV,QAAQ,WAAW,MAAM;YAC3B;YACA,OAAO;QACT,EAAE,OAAO,OAAO;YACd,IAAI,iBAAiB,SAAS,MAAM,IAAI,KAAK,cAAc;gBACzD,MAAM,IAAI,MAAM,CAAC,wBAAwB,EAAE,IAAI,CAAC,OAAO,CAAC,EAAE,CAAC;YAC7D;YACA,MAAM;QACR,SAAU;YACR,aAAa;QACf;IACF;AACF;AAEA,+EAA+E;AAC/E,qBAAqB;AACrB,+EAA+E;AAE/E,IAAI,gBAAqC;AAMlC,SAAS,gBAAgB,MAA8B;IAC5D,IAAI,CAAC,iBAAiB,QAAQ;QAC5B,gBAAgB,IAAI,aAAa;YAC/B,SAAS,QAAQ,WAAW,QAAQ,GAAG,CAAC,eAAe,IAAI;YAC3D,SAAS,QAAQ,WAAW;YAC5B,cAAc,QAAQ,gBAAgB,QAAQ,GAAG,CAAC,oBAAoB,IAAI;QAC5E;IACF;IACA,OAAO;AACT;AAKO,eAAe,kBAAkB,OAAgB;IACtD,MAAM,SAAS,IAAI,aAAa;QAC9B,SAAS,WAAW,QAAQ,GAAG,CAAC,eAAe,IAAI;QACnD,SAAS;IACX;IACA,OAAO,OAAO,MAAM;AACtB"}},
    {"offset": {"line": 357, "column": 0}, "map": {"version":3,"sources":["file:///Users/jamesspalding/OpenClaw-OS/src/lib/ollama/index.ts"],"sourcesContent":["/**\n * Ollama Module - Local LLM Integration\n */\n\nexport {\n  OllamaClient,\n  getOllamaClient,\n  checkOllamaHealth,\n  type OllamaConfig,\n  type OllamaModel,\n  type OllamaMessage,\n  type OllamaChatRequest,\n  type OllamaChatResponse,\n  type OllamaGenerateRequest,\n  type OllamaGenerateResponse,\n  type OllamaHealthStatus,\n} from './client';\n"],"names":[],"mappings":";AAAA;;CAEC,GAED"}},
    {"offset": {"line": 366, "column": 0}, "map": {"version":3,"sources":["file:///Users/jamesspalding/OpenClaw-OS/src/lib/lynkr/client.ts"],"sourcesContent":["/**\n * Lynkr Client - Universal LLM Proxy Integration\n *\n * Lynkr is a self-hosted proxy that routes AI requests to:\n * - Local models (Ollama, llama.cpp, LM Studio)\n * - Cloud providers (OpenRouter, Anthropic, OpenAI, AWS Bedrock, etc.)\n *\n * This client connects to a Lynkr instance (local or via tunnel) and\n * sends requests in Anthropic format, which Lynkr converts as needed.\n *\n * SECURITY:\n * - API key required (min 32 chars for production)\n * - SSRF protection blocks internal IPs\n * - Audit logging for all requests\n */\n\n// ============================================================================\n// Security Constants\n// ============================================================================\n\n/** Minimum API key length for production use */\nconst MIN_API_KEY_LENGTH = 32;\n\n/** Internal IP patterns that should be blocked (SSRF protection) */\nconst BLOCKED_IP_PATTERNS = [\n  /^127\\./,                           // Loopback\n  /^10\\./,                            // Private Class A\n  /^172\\.(1[6-9]|2[0-9]|3[0-1])\\./,   // Private Class B\n  /^192\\.168\\./,                      // Private Class C\n  /^169\\.254\\./,                      // Link-local\n  /^0\\./,                             // \"This\" network\n  /^100\\.(6[4-9]|[7-9][0-9]|1[0-2][0-7])\\./,  // Carrier-grade NAT\n  /^::1$/,                            // IPv6 loopback\n  /^fc00:/i,                          // IPv6 unique local\n  /^fe80:/i,                          // IPv6 link-local\n  /^localhost$/i,                     // Hostname\n  /^.*\\.local$/i,                     // mDNS\n  /metadata\\.google\\.internal/i,      // GCP metadata\n  /169\\.254\\.169\\.254/,               // AWS/GCP/Azure metadata\n];\n\n/** Allowed URL schemes */\nconst ALLOWED_SCHEMES = ['https:', 'http:'];\n\nexport interface LynkrConfig {\n  /** Lynkr endpoint URL (e.g., http://localhost:8081 or https://tunnel.example.com) */\n  baseUrl: string;\n  /** API key - required for production (min 32 chars) */\n  apiKey?: string;\n  /** Request timeout in ms */\n  timeout?: number;\n  /** Default model to use (pinned to gpt-oss:20b) */\n  defaultModel?: string;\n  /** Skip API key validation (for local dev only) */\n  skipApiKeyValidation?: boolean;\n  /** Skip SSRF validation (for local dev only) */\n  skipSsrfValidation?: boolean;\n}\n\nexport interface LynkrMessage {\n  role: 'user' | 'assistant' | 'system';\n  content: string;\n}\n\nexport interface LynkrTool {\n  name: string;\n  description: string;\n  input_schema: {\n    type: 'object';\n    properties: Record<string, unknown>;\n    required?: string[];\n  };\n}\n\nexport interface LynkrChatRequest {\n  model: string;\n  messages: LynkrMessage[];\n  max_tokens?: number;\n  temperature?: number;\n  tools?: LynkrTool[];\n  system?: string;\n  stream?: boolean;\n}\n\nexport interface LynkrToolUse {\n  type: 'tool_use';\n  id: string;\n  name: string;\n  input: Record<string, unknown>;\n}\n\nexport interface LynkrTextContent {\n  type: 'text';\n  text: string;\n}\n\nexport type LynkrContentBlock = LynkrTextContent | LynkrToolUse;\n\nexport interface LynkrChatResponse {\n  id: string;\n  type: 'message';\n  role: 'assistant';\n  model: string;\n  content: LynkrContentBlock[];\n  stop_reason: 'end_turn' | 'tool_use' | 'max_tokens' | 'stop_sequence';\n  stop_sequence?: string | null;\n  usage: {\n    input_tokens: number;\n    output_tokens: number;\n    cache_creation_input_tokens?: number;\n    cache_read_input_tokens?: number;\n  };\n}\n\nexport interface LynkrHealthStatus {\n  connected: boolean;\n  latencyMs?: number;\n  error?: string;\n  provider?: string;\n  model?: string;\n  version?: string;\n  features?: {\n    memory?: boolean;\n    tools?: boolean;\n    streaming?: boolean;\n    embeddings?: boolean;\n  };\n}\n\nexport interface LynkrMetrics {\n  requestCount: number;\n  totalTokens: number;\n  avgLatency: number;\n  routingDecisions: {\n    local: number;\n    cloud: number;\n  };\n}\n\n// ============================================================================\n// Security Validation\n// ============================================================================\n\nexport interface LynkrSecurityError extends Error {\n  code: 'INVALID_API_KEY' | 'SSRF_BLOCKED' | 'INVALID_URL';\n}\n\n/**\n * Validate API key meets minimum security requirements\n */\nexport function validateApiKey(apiKey: string | undefined, skipValidation?: boolean): void {\n  if (skipValidation) return;\n\n  if (!apiKey || apiKey.length < MIN_API_KEY_LENGTH) {\n    const error = new Error(\n      `Lynkr API key must be at least ${MIN_API_KEY_LENGTH} characters. ` +\n      `Generate with: openssl rand -base64 48`\n    ) as LynkrSecurityError;\n    error.code = 'INVALID_API_KEY';\n    throw error;\n  }\n\n  // Block obviously insecure default keys\n  const insecureKeys = ['lynkr-local', 'dummy', 'test', 'local', 'dev'];\n  if (insecureKeys.includes(apiKey.toLowerCase())) {\n    const error = new Error(\n      'Lynkr API key cannot be a default/test value. Generate a secure key.'\n    ) as LynkrSecurityError;\n    error.code = 'INVALID_API_KEY';\n    throw error;\n  }\n}\n\n/**\n * Validate URL is not targeting internal networks (SSRF protection)\n */\nexport function validateUrlSafety(urlString: string, skipValidation?: boolean): void {\n  if (skipValidation) return;\n\n  try {\n    const url = new URL(urlString);\n\n    // Check scheme\n    if (!ALLOWED_SCHEMES.includes(url.protocol)) {\n      const error = new Error(\n        `URL scheme ${url.protocol} not allowed. Use http: or https:`\n      ) as LynkrSecurityError;\n      error.code = 'INVALID_URL';\n      throw error;\n    }\n\n    // Check hostname against blocked patterns\n    const hostname = url.hostname;\n    for (const pattern of BLOCKED_IP_PATTERNS) {\n      if (pattern.test(hostname)) {\n        const error = new Error(\n          `SSRF protection: ${hostname} is not allowed. Internal IPs are blocked.`\n        ) as LynkrSecurityError;\n        error.code = 'SSRF_BLOCKED';\n        throw error;\n      }\n    }\n  } catch (e) {\n    if ((e as LynkrSecurityError).code) throw e;\n\n    const error = new Error(`Invalid URL: ${urlString}`) as LynkrSecurityError;\n    error.code = 'INVALID_URL';\n    throw error;\n  }\n}\n\n/**\n * Check if running in local development mode\n */\nfunction isLocalDev(): boolean {\n  if (typeof window !== 'undefined') return false;\n  return process.env.NODE_ENV === 'development' ||\n         process.env.LYNKR_SKIP_SECURITY === 'true';\n}\n\n// ============================================================================\n// Audit Logging\n// ============================================================================\n\nexport interface LynkrAuditLog {\n  timestamp: number;\n  action: 'chat' | 'stream' | 'health' | 'metrics';\n  model?: string;\n  baseUrl: string;\n  latencyMs?: number;\n  success: boolean;\n  error?: string;\n  inputTokens?: number;\n  outputTokens?: number;\n}\n\n/** Audit log callback - set this to capture all Lynkr requests */\nexport let auditLogCallback: ((log: LynkrAuditLog) => void) | null = null;\n\n/**\n * Set the audit log callback for security monitoring\n */\nexport function setAuditLogCallback(callback: ((log: LynkrAuditLog) => void) | null): void {\n  auditLogCallback = callback;\n}\n\nfunction logAudit(log: LynkrAuditLog): void {\n  if (auditLogCallback) {\n    try {\n      auditLogCallback(log);\n    } catch (error) {\n      // Log audit failures to console as fallback - don't lose security events silently\n      console.error('[Lynkr AUDIT FAILURE] Failed to log audit event:', error);\n      console.error('[Lynkr AUDIT FAILURE] Event data:', JSON.stringify(log));\n    }\n  }\n\n  // Also log to console in development\n  if (isLocalDev()) {\n    console.log('[Lynkr Audit]', JSON.stringify(log));\n  }\n}\n\n// ============================================================================\n// Lynkr Client\n// ============================================================================\n\n/**\n * Lynkr Client\n *\n * Connects to a Lynkr proxy instance and sends AI requests.\n * Lynkr handles routing to the appropriate provider (local or cloud).\n *\n * SECURITY FEATURES:\n * - API key validation (min 32 chars)\n * - SSRF protection (blocks internal IPs)\n * - Audit logging for all requests\n */\nexport class LynkrClient {\n  private config: Required<Omit<LynkrConfig, 'skipApiKeyValidation' | 'skipSsrfValidation'>> & {\n    skipApiKeyValidation: boolean;\n    skipSsrfValidation: boolean;\n  };\n\n  constructor(config: LynkrConfig) {\n    const skipSecurity = isLocalDev();\n    const skipApiKeyValidation = config.skipApiKeyValidation ?? skipSecurity;\n    const skipSsrfValidation = config.skipSsrfValidation ?? skipSecurity;\n\n    // Validate security requirements\n    if (!skipApiKeyValidation) {\n      validateApiKey(config.apiKey, false);\n    }\n\n    if (!skipSsrfValidation) {\n      validateUrlSafety(config.baseUrl, false);\n    }\n\n    this.config = {\n      baseUrl: config.baseUrl.replace(/\\/$/, ''), // Remove trailing slash\n      apiKey: config.apiKey ?? '',\n      timeout: config.timeout ?? 120000, // 2 minutes default\n      defaultModel: config.defaultModel ?? 'gpt-oss:20b', // Pinned to local model\n      skipApiKeyValidation,\n      skipSsrfValidation,\n    };\n  }\n\n  /**\n   * Check if Lynkr is healthy and reachable\n   */\n  async checkHealth(): Promise<LynkrHealthStatus> {\n    const start = Date.now();\n\n    try {\n      const controller = new AbortController();\n      const timeoutId = setTimeout(() => controller.abort(), 5000);\n\n      const response = await fetch(`${this.config.baseUrl}/health`, {\n        method: 'GET',\n        signal: controller.signal,\n      });\n\n      clearTimeout(timeoutId);\n\n      if (!response.ok) {\n        return {\n          connected: false,\n          latencyMs: Date.now() - start,\n          error: `HTTP ${response.status}: ${response.statusText}`,\n        };\n      }\n\n      const data = await response.json();\n\n      return {\n        connected: true,\n        latencyMs: Date.now() - start,\n        provider: data.provider,\n        model: data.model,\n        version: data.version,\n        features: {\n          memory: data.memory?.enabled ?? false,\n          tools: data.tools?.enabled ?? true,\n          streaming: data.streaming?.enabled ?? true,\n          embeddings: data.embeddings?.enabled ?? false,\n        },\n      };\n    } catch (error) {\n      return {\n        connected: false,\n        latencyMs: Date.now() - start,\n        error: error instanceof Error ? error.message : 'Connection failed',\n      };\n    }\n  }\n\n  /**\n   * Get Lynkr metrics\n   */\n  async getMetrics(): Promise<LynkrMetrics | null> {\n    try {\n      const response = await fetch(`${this.config.baseUrl}/metrics`, {\n        method: 'GET',\n        signal: AbortSignal.timeout(5000),\n      });\n\n      if (!response.ok) return null;\n\n      return await response.json();\n    } catch {\n      return null;\n    }\n  }\n\n  /**\n   * Send a chat request through Lynkr\n   * Uses Anthropic message format (Lynkr converts as needed)\n   */\n  async chat(request: Omit<LynkrChatRequest, 'model'> & { model?: string }): Promise<LynkrChatResponse> {\n    const start = Date.now();\n    const model = request.model ?? this.config.defaultModel;\n\n    const body: LynkrChatRequest = {\n      model,\n      messages: request.messages,\n      max_tokens: request.max_tokens ?? 4096,\n      temperature: request.temperature ?? 0.7,\n      tools: request.tools,\n      system: request.system,\n      stream: false,\n    };\n\n    const controller = new AbortController();\n    const timeoutId = setTimeout(() => controller.abort(), this.config.timeout);\n\n    try {\n      const response = await fetch(`${this.config.baseUrl}/v1/messages`, {\n        method: 'POST',\n        headers: {\n          'Content-Type': 'application/json',\n          'x-api-key': this.config.apiKey,\n          'anthropic-version': '2023-06-01',\n        },\n        body: JSON.stringify(body),\n        signal: controller.signal,\n      });\n\n      clearTimeout(timeoutId);\n\n      if (!response.ok) {\n        const errorText = await response.text();\n        const error = new Error(`Lynkr request failed: ${response.status} - ${errorText}`);\n\n        logAudit({\n          timestamp: Date.now(),\n          action: 'chat',\n          model,\n          baseUrl: this.config.baseUrl,\n          latencyMs: Date.now() - start,\n          success: false,\n          error: error.message,\n        });\n\n        throw error;\n      }\n\n      const result: LynkrChatResponse = await response.json();\n\n      logAudit({\n        timestamp: Date.now(),\n        action: 'chat',\n        model,\n        baseUrl: this.config.baseUrl,\n        latencyMs: Date.now() - start,\n        success: true,\n        inputTokens: result.usage?.input_tokens,\n        outputTokens: result.usage?.output_tokens,\n      });\n\n      return result;\n    } catch (error) {\n      clearTimeout(timeoutId);\n\n      // Log if not already logged\n      if (!(error instanceof Error && error.message.includes('Lynkr request failed'))) {\n        logAudit({\n          timestamp: Date.now(),\n          action: 'chat',\n          model,\n          baseUrl: this.config.baseUrl,\n          latencyMs: Date.now() - start,\n          success: false,\n          error: error instanceof Error ? error.message : 'Unknown error',\n        });\n      }\n\n      throw error;\n    }\n  }\n\n  /**\n   * Send a streaming chat request through Lynkr\n   * Returns an async generator of content deltas\n   */\n  async *chatStream(\n    request: Omit<LynkrChatRequest, 'model' | 'stream'> & { model?: string }\n  ): AsyncGenerator<{\n    type: 'content_block_delta' | 'message_start' | 'message_stop' | 'content_block_start' | 'content_block_stop';\n    delta?: { type: 'text_delta'; text: string } | { type: 'tool_use'; id: string; name: string; input: string };\n    index?: number;\n    content_block?: LynkrContentBlock;\n    message?: Partial<LynkrChatResponse>;\n  }> {\n    const body: LynkrChatRequest = {\n      model: request.model ?? this.config.defaultModel,\n      messages: request.messages,\n      max_tokens: request.max_tokens ?? 4096,\n      temperature: request.temperature ?? 0.7,\n      tools: request.tools,\n      system: request.system,\n      stream: true,\n    };\n\n    const controller = new AbortController();\n    const timeoutId = setTimeout(() => controller.abort(), this.config.timeout);\n\n    try {\n      const response = await fetch(`${this.config.baseUrl}/v1/messages`, {\n        method: 'POST',\n        headers: {\n          'Content-Type': 'application/json',\n          'x-api-key': this.config.apiKey,\n          'anthropic-version': '2023-06-01',\n        },\n        body: JSON.stringify(body),\n        signal: controller.signal,\n      });\n\n      clearTimeout(timeoutId);\n\n      if (!response.ok) {\n        const errorText = await response.text();\n        throw new Error(`Lynkr stream failed: ${response.status} - ${errorText}`);\n      }\n\n      if (!response.body) {\n        throw new Error('No response body for streaming');\n      }\n\n      const reader = response.body.getReader();\n      const decoder = new TextDecoder();\n      let buffer = '';\n\n      while (true) {\n        const { done, value } = await reader.read();\n        if (done) break;\n\n        buffer += decoder.decode(value, { stream: true });\n        const lines = buffer.split('\\n');\n        buffer = lines.pop() ?? '';\n\n        for (const line of lines) {\n          if (line.startsWith('data: ')) {\n            const data = line.slice(6).trim();\n            if (data === '[DONE]') return;\n\n            try {\n              const event = JSON.parse(data);\n              yield event;\n            } catch {\n              // Skip invalid JSON\n            }\n          }\n        }\n      }\n    } catch (error) {\n      clearTimeout(timeoutId);\n      throw error;\n    }\n  }\n\n  /**\n   * Extract text content from a Lynkr response\n   */\n  static extractText(response: LynkrChatResponse): string {\n    return response.content\n      .filter((block): block is LynkrTextContent => block.type === 'text')\n      .map((block) => block.text)\n      .join('');\n  }\n\n  /**\n   * Extract tool calls from a Lynkr response\n   */\n  static extractToolCalls(response: LynkrChatResponse): LynkrToolUse[] {\n    return response.content.filter((block): block is LynkrToolUse => block.type === 'tool_use');\n  }\n\n  /**\n   * Check if response contains tool calls\n   */\n  static hasToolCalls(response: LynkrChatResponse): boolean {\n    return response.stop_reason === 'tool_use' || response.content.some((block) => block.type === 'tool_use');\n  }\n}\n\n/**\n * Get a configured Lynkr client\n *\n * SECURITY: In production, requires:\n * - LYNKR_API_KEY env var with min 32 chars\n * - Non-internal baseUrl (SSRF protection)\n *\n * In development (NODE_ENV=development), security checks are relaxed.\n */\nexport function getLynkrClient(config?: Partial<LynkrConfig>): LynkrClient {\n  // Environment variables are only available on the server\n  const env = typeof window === 'undefined' ? {\n    baseUrl: process.env.LYNKR_BASE_URL,\n    apiKey: process.env.LYNKR_API_KEY,\n    timeout: process.env.LYNKR_TIMEOUT,\n    defaultModel: process.env.LYNKR_DEFAULT_MODEL,\n    skipSecurity: process.env.LYNKR_SKIP_SECURITY === 'true' ||\n                  process.env.NODE_ENV === 'development',\n  } : { skipSecurity: false };\n\n  const skipSecurity = config?.skipApiKeyValidation ?? config?.skipSsrfValidation ?? env.skipSecurity;\n\n  return new LynkrClient({\n    baseUrl: config?.baseUrl ?? env.baseUrl ?? 'http://localhost:8081',\n    apiKey: config?.apiKey ?? env.apiKey,\n    timeout: config?.timeout ?? parseInt(env.timeout ?? '120000', 10),\n    defaultModel: config?.defaultModel ?? env.defaultModel ?? 'gpt-oss:20b', // Pinned to local model\n    skipApiKeyValidation: skipSecurity,\n    skipSsrfValidation: skipSecurity,\n  });\n}\n\n/**\n * Check Lynkr health with a simple function\n *\n * Health checks skip security validation because:\n * 1. They don't require authentication (just checking connectivity)\n * 2. They may use tunnel URLs that would fail SSRF checks\n */\nexport async function checkLynkrHealth(baseUrl?: string): Promise<LynkrHealthStatus> {\n  // Health checks skip security validation - no auth needed for connectivity test\n  const client = getLynkrClient({\n    baseUrl,\n    skipApiKeyValidation: true,\n    skipSsrfValidation: true,\n  });\n  return client.checkHealth();\n}\n"],"names":[],"mappings":";;;;;;;;;;;;;;;;AAAA;;;;;;;;;;;;;;CAcC,GAED,+EAA+E;AAC/E,qBAAqB;AACrB,+EAA+E;AAE/E,8CAA8C,GAC9C,MAAM,qBAAqB;AAE3B,kEAAkE,GAClE,MAAM,sBAAsB;IAC1B;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA;CACD;AAED,wBAAwB,GACxB,MAAM,kBAAkB;IAAC;IAAU;CAAQ;AA4GpC,SAAS,eAAe,MAA0B,EAAE,cAAwB;IACjF,IAAI,gBAAgB;IAEpB,IAAI,CAAC,UAAU,OAAO,MAAM,GAAG,oBAAoB;QACjD,MAAM,QAAQ,IAAI,MAChB,CAAC,+BAA+B,EAAE,mBAAmB,aAAa,CAAC,GACnE,CAAC,sCAAsC,CAAC;QAE1C,MAAM,IAAI,GAAG;QACb,MAAM;IACR;IAEA,wCAAwC;IACxC,MAAM,eAAe;QAAC;QAAe;QAAS;QAAQ;QAAS;KAAM;IACrE,IAAI,aAAa,QAAQ,CAAC,OAAO,WAAW,KAAK;QAC/C,MAAM,QAAQ,IAAI,MAChB;QAEF,MAAM,IAAI,GAAG;QACb,MAAM;IACR;AACF;AAKO,SAAS,kBAAkB,SAAiB,EAAE,cAAwB;IAC3E,IAAI,gBAAgB;IAEpB,IAAI;QACF,MAAM,MAAM,IAAI,IAAI;QAEpB,eAAe;QACf,IAAI,CAAC,gBAAgB,QAAQ,CAAC,IAAI,QAAQ,GAAG;YAC3C,MAAM,QAAQ,IAAI,MAChB,CAAC,WAAW,EAAE,IAAI,QAAQ,CAAC,iCAAiC,CAAC;YAE/D,MAAM,IAAI,GAAG;YACb,MAAM;QACR;QAEA,0CAA0C;QAC1C,MAAM,WAAW,IAAI,QAAQ;QAC7B,KAAK,MAAM,WAAW,oBAAqB;YACzC,IAAI,QAAQ,IAAI,CAAC,WAAW;gBAC1B,MAAM,QAAQ,IAAI,MAChB,CAAC,iBAAiB,EAAE,SAAS,0CAA0C,CAAC;gBAE1E,MAAM,IAAI,GAAG;gBACb,MAAM;YACR;QACF;IACF,EAAE,OAAO,GAAG;QACV,IAAI,AAAC,EAAyB,IAAI,EAAE,MAAM;QAE1C,MAAM,QAAQ,IAAI,MAAM,CAAC,aAAa,EAAE,WAAW;QACnD,MAAM,IAAI,GAAG;QACb,MAAM;IACR;AACF;AAEA;;CAEC,GACD,SAAS;IACP;;IACA,OAAO,oDAAyB,iBACzB,QAAQ,GAAG,CAAC,mBAAmB,KAAK;AAC7C;AAmBO,IAAI,mBAA0D;AAK9D,SAAS,oBAAoB,QAA+C;IACjF,mBAAmB;AACrB;AAEA,SAAS,SAAS,GAAkB;IAClC,IAAI,kBAAkB;QACpB,IAAI;YACF,iBAAiB;QACnB,EAAE,OAAO,OAAO;YACd,kFAAkF;YAClF,QAAQ,KAAK,CAAC,oDAAoD;YAClE,QAAQ,KAAK,CAAC,qCAAqC,KAAK,SAAS,CAAC;QACpE;IACF;IAEA,qCAAqC;IACrC,IAAI,cAAc;QAChB,QAAQ,GAAG,CAAC,iBAAiB,KAAK,SAAS,CAAC;IAC9C;AACF;AAiBO,MAAM;IACH,OAGN;IAEF,YAAY,MAAmB,CAAE;QAC/B,MAAM,eAAe;QACrB,MAAM,uBAAuB,OAAO,oBAAoB,IAAI;QAC5D,MAAM,qBAAqB,OAAO,kBAAkB,IAAI;QAExD,iCAAiC;QACjC,IAAI,CAAC,sBAAsB;YACzB,eAAe,OAAO,MAAM,EAAE;QAChC;QAEA,IAAI,CAAC,oBAAoB;YACvB,kBAAkB,OAAO,OAAO,EAAE;QACpC;QAEA,IAAI,CAAC,MAAM,GAAG;YACZ,SAAS,OAAO,OAAO,CAAC,OAAO,CAAC,OAAO;YACvC,QAAQ,OAAO,MAAM,IAAI;YACzB,SAAS,OAAO,OAAO,IAAI;YAC3B,cAAc,OAAO,YAAY,IAAI;YACrC;YACA;QACF;IACF;IAEA;;GAEC,GACD,MAAM,cAA0C;QAC9C,MAAM,QAAQ,KAAK,GAAG;QAEtB,IAAI;YACF,MAAM,aAAa,IAAI;YACvB,MAAM,YAAY,WAAW,IAAM,WAAW,KAAK,IAAI;YAEvD,MAAM,WAAW,MAAM,MAAM,GAAG,IAAI,CAAC,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,EAAE;gBAC5D,QAAQ;gBACR,QAAQ,WAAW,MAAM;YAC3B;YAEA,aAAa;YAEb,IAAI,CAAC,SAAS,EAAE,EAAE;gBAChB,OAAO;oBACL,WAAW;oBACX,WAAW,KAAK,GAAG,KAAK;oBACxB,OAAO,CAAC,KAAK,EAAE,SAAS,MAAM,CAAC,EAAE,EAAE,SAAS,UAAU,EAAE;gBAC1D;YACF;YAEA,MAAM,OAAO,MAAM,SAAS,IAAI;YAEhC,OAAO;gBACL,WAAW;gBACX,WAAW,KAAK,GAAG,KAAK;gBACxB,UAAU,KAAK,QAAQ;gBACvB,OAAO,KAAK,KAAK;gBACjB,SAAS,KAAK,OAAO;gBACrB,UAAU;oBACR,QAAQ,KAAK,MAAM,EAAE,WAAW;oBAChC,OAAO,KAAK,KAAK,EAAE,WAAW;oBAC9B,WAAW,KAAK,SAAS,EAAE,WAAW;oBACtC,YAAY,KAAK,UAAU,EAAE,WAAW;gBAC1C;YACF;QACF,EAAE,OAAO,OAAO;YACd,OAAO;gBACL,WAAW;gBACX,WAAW,KAAK,GAAG,KAAK;gBACxB,OAAO,iBAAiB,QAAQ,MAAM,OAAO,GAAG;YAClD;QACF;IACF;IAEA;;GAEC,GACD,MAAM,aAA2C;QAC/C,IAAI;YACF,MAAM,WAAW,MAAM,MAAM,GAAG,IAAI,CAAC,MAAM,CAAC,OAAO,CAAC,QAAQ,CAAC,EAAE;gBAC7D,QAAQ;gBACR,QAAQ,YAAY,OAAO,CAAC;YAC9B;YAEA,IAAI,CAAC,SAAS,EAAE,EAAE,OAAO;YAEzB,OAAO,MAAM,SAAS,IAAI;QAC5B,EAAE,OAAM;YACN,OAAO;QACT;IACF;IAEA;;;GAGC,GACD,MAAM,KAAK,OAA6D,EAA8B;QACpG,MAAM,QAAQ,KAAK,GAAG;QACtB,MAAM,QAAQ,QAAQ,KAAK,IAAI,IAAI,CAAC,MAAM,CAAC,YAAY;QAEvD,MAAM,OAAyB;YAC7B;YACA,UAAU,QAAQ,QAAQ;YAC1B,YAAY,QAAQ,UAAU,IAAI;YAClC,aAAa,QAAQ,WAAW,IAAI;YACpC,OAAO,QAAQ,KAAK;YACpB,QAAQ,QAAQ,MAAM;YACtB,QAAQ;QACV;QAEA,MAAM,aAAa,IAAI;QACvB,MAAM,YAAY,WAAW,IAAM,WAAW,KAAK,IAAI,IAAI,CAAC,MAAM,CAAC,OAAO;QAE1E,IAAI;YACF,MAAM,WAAW,MAAM,MAAM,GAAG,IAAI,CAAC,MAAM,CAAC,OAAO,CAAC,YAAY,CAAC,EAAE;gBACjE,QAAQ;gBACR,SAAS;oBACP,gBAAgB;oBAChB,aAAa,IAAI,CAAC,MAAM,CAAC,MAAM;oBAC/B,qBAAqB;gBACvB;gBACA,MAAM,KAAK,SAAS,CAAC;gBACrB,QAAQ,WAAW,MAAM;YAC3B;YAEA,aAAa;YAEb,IAAI,CAAC,SAAS,EAAE,EAAE;gBAChB,MAAM,YAAY,MAAM,SAAS,IAAI;gBACrC,MAAM,QAAQ,IAAI,MAAM,CAAC,sBAAsB,EAAE,SAAS,MAAM,CAAC,GAAG,EAAE,WAAW;gBAEjF,SAAS;oBACP,WAAW,KAAK,GAAG;oBACnB,QAAQ;oBACR;oBACA,SAAS,IAAI,CAAC,MAAM,CAAC,OAAO;oBAC5B,WAAW,KAAK,GAAG,KAAK;oBACxB,SAAS;oBACT,OAAO,MAAM,OAAO;gBACtB;gBAEA,MAAM;YACR;YAEA,MAAM,SAA4B,MAAM,SAAS,IAAI;YAErD,SAAS;gBACP,WAAW,KAAK,GAAG;gBACnB,QAAQ;gBACR;gBACA,SAAS,IAAI,CAAC,MAAM,CAAC,OAAO;gBAC5B,WAAW,KAAK,GAAG,KAAK;gBACxB,SAAS;gBACT,aAAa,OAAO,KAAK,EAAE;gBAC3B,cAAc,OAAO,KAAK,EAAE;YAC9B;YAEA,OAAO;QACT,EAAE,OAAO,OAAO;YACd,aAAa;YAEb,4BAA4B;YAC5B,IAAI,CAAC,CAAC,iBAAiB,SAAS,MAAM,OAAO,CAAC,QAAQ,CAAC,uBAAuB,GAAG;gBAC/E,SAAS;oBACP,WAAW,KAAK,GAAG;oBACnB,QAAQ;oBACR;oBACA,SAAS,IAAI,CAAC,MAAM,CAAC,OAAO;oBAC5B,WAAW,KAAK,GAAG,KAAK;oBACxB,SAAS;oBACT,OAAO,iBAAiB,QAAQ,MAAM,OAAO,GAAG;gBAClD;YACF;YAEA,MAAM;QACR;IACF;IAEA;;;GAGC,GACD,OAAO,WACL,OAAwE,EAOvE;QACD,MAAM,OAAyB;YAC7B,OAAO,QAAQ,KAAK,IAAI,IAAI,CAAC,MAAM,CAAC,YAAY;YAChD,UAAU,QAAQ,QAAQ;YAC1B,YAAY,QAAQ,UAAU,IAAI;YAClC,aAAa,QAAQ,WAAW,IAAI;YACpC,OAAO,QAAQ,KAAK;YACpB,QAAQ,QAAQ,MAAM;YACtB,QAAQ;QACV;QAEA,MAAM,aAAa,IAAI;QACvB,MAAM,YAAY,WAAW,IAAM,WAAW,KAAK,IAAI,IAAI,CAAC,MAAM,CAAC,OAAO;QAE1E,IAAI;YACF,MAAM,WAAW,MAAM,MAAM,GAAG,IAAI,CAAC,MAAM,CAAC,OAAO,CAAC,YAAY,CAAC,EAAE;gBACjE,QAAQ;gBACR,SAAS;oBACP,gBAAgB;oBAChB,aAAa,IAAI,CAAC,MAAM,CAAC,MAAM;oBAC/B,qBAAqB;gBACvB;gBACA,MAAM,KAAK,SAAS,CAAC;gBACrB,QAAQ,WAAW,MAAM;YAC3B;YAEA,aAAa;YAEb,IAAI,CAAC,SAAS,EAAE,EAAE;gBAChB,MAAM,YAAY,MAAM,SAAS,IAAI;gBACrC,MAAM,IAAI,MAAM,CAAC,qBAAqB,EAAE,SAAS,MAAM,CAAC,GAAG,EAAE,WAAW;YAC1E;YAEA,IAAI,CAAC,SAAS,IAAI,EAAE;gBAClB,MAAM,IAAI,MAAM;YAClB;YAEA,MAAM,SAAS,SAAS,IAAI,CAAC,SAAS;YACtC,MAAM,UAAU,IAAI;YACpB,IAAI,SAAS;YAEb,MAAO,KAAM;gBACX,MAAM,EAAE,IAAI,EAAE,KAAK,EAAE,GAAG,MAAM,OAAO,IAAI;gBACzC,IAAI,MAAM;gBAEV,UAAU,QAAQ,MAAM,CAAC,OAAO;oBAAE,QAAQ;gBAAK;gBAC/C,MAAM,QAAQ,OAAO,KAAK,CAAC;gBAC3B,SAAS,MAAM,GAAG,MAAM;gBAExB,KAAK,MAAM,QAAQ,MAAO;oBACxB,IAAI,KAAK,UAAU,CAAC,WAAW;wBAC7B,MAAM,OAAO,KAAK,KAAK,CAAC,GAAG,IAAI;wBAC/B,IAAI,SAAS,UAAU;wBAEvB,IAAI;4BACF,MAAM,QAAQ,KAAK,KAAK,CAAC;4BACzB,MAAM;wBACR,EAAE,OAAM;wBACN,oBAAoB;wBACtB;oBACF;gBACF;YACF;QACF,EAAE,OAAO,OAAO;YACd,aAAa;YACb,MAAM;QACR;IACF;IAEA;;GAEC,GACD,OAAO,YAAY,QAA2B,EAAU;QACtD,OAAO,SAAS,OAAO,CACpB,MAAM,CAAC,CAAC,QAAqC,MAAM,IAAI,KAAK,QAC5D,GAAG,CAAC,CAAC,QAAU,MAAM,IAAI,EACzB,IAAI,CAAC;IACV;IAEA;;GAEC,GACD,OAAO,iBAAiB,QAA2B,EAAkB;QACnE,OAAO,SAAS,OAAO,CAAC,MAAM,CAAC,CAAC,QAAiC,MAAM,IAAI,KAAK;IAClF;IAEA;;GAEC,GACD,OAAO,aAAa,QAA2B,EAAW;QACxD,OAAO,SAAS,WAAW,KAAK,cAAc,SAAS,OAAO,CAAC,IAAI,CAAC,CAAC,QAAU,MAAM,IAAI,KAAK;IAChG;AACF;AAWO,SAAS,eAAe,MAA6B;IAC1D,yDAAyD;IACzD,MAAM,MAAM,uCAAgC;QAC1C,SAAS,QAAQ,GAAG,CAAC,cAAc;QACnC,QAAQ,QAAQ,GAAG,CAAC,aAAa;QACjC,SAAS,QAAQ,GAAG,CAAC,aAAa;QAClC,cAAc,QAAQ,GAAG,CAAC,mBAAmB;QAC7C,cAAc,QAAQ,GAAG,CAAC,mBAAmB,KAAK,UACpC,oDAAyB;IACzC,IAAI;IAEJ,MAAM,eAAe,QAAQ,wBAAwB,QAAQ,sBAAsB,IAAI,YAAY;IAEnG,OAAO,IAAI,YAAY;QACrB,SAAS,QAAQ,WAAW,IAAI,OAAO,IAAI;QAC3C,QAAQ,QAAQ,UAAU,IAAI,MAAM;QACpC,SAAS,QAAQ,WAAW,SAAS,IAAI,OAAO,IAAI,UAAU;QAC9D,cAAc,QAAQ,gBAAgB,IAAI,YAAY,IAAI;QAC1D,sBAAsB;QACtB,oBAAoB;IACtB;AACF;AASO,eAAe,iBAAiB,OAAgB;IACrD,gFAAgF;IAChF,MAAM,SAAS,eAAe;QAC5B;QACA,sBAAsB;QACtB,oBAAoB;IACtB;IACA,OAAO,OAAO,WAAW;AAC3B"}},
    {"offset": {"line": 753, "column": 0}, "map": {"version":3,"sources":["file:///Users/jamesspalding/OpenClaw-OS/src/lib/lynkr/index.ts"],"sourcesContent":["/**\n * Lynkr Module - Universal LLM Proxy Integration\n *\n * Lynkr enables OpenClaw-OS to use local models from anywhere:\n * 1. Run Ollama + Lynkr on your Mac\n * 2. Expose Lynkr via Cloudflare Tunnel\n * 3. OpenClaw-OS (production) connects to your tunnel\n * 4. Claw AI uses your local models, from anywhere\n *\n * SECURITY:\n * - API key required (min 32 chars for production)\n * - SSRF protection blocks internal IPs\n * - Audit logging for all requests\n */\n\nexport {\n  // Client\n  LynkrClient,\n  getLynkrClient,\n  checkLynkrHealth,\n\n  // Security\n  validateApiKey,\n  validateUrlSafety,\n  setAuditLogCallback,\n\n  // Types\n  type LynkrConfig,\n  type LynkrMessage,\n  type LynkrTool,\n  type LynkrChatRequest,\n  type LynkrChatResponse,\n  type LynkrHealthStatus,\n  type LynkrMetrics,\n  type LynkrToolUse,\n  type LynkrTextContent,\n  type LynkrContentBlock,\n  type LynkrSecurityError,\n  type LynkrAuditLog,\n} from './client';\n"],"names":[],"mappings":";AAAA;;;;;;;;;;;;;CAaC,GAED"}},
    {"offset": {"line": 773, "column": 0}, "map": {"version":3,"sources":["file:///Users/jamesspalding/OpenClaw-OS/src/app/api/health/providers/route.ts"],"sourcesContent":["/**\n * AI Provider Health Check Endpoint - Phase 3 Local LLM Integration\n *\n * GET /api/health/providers\n *\n * Returns connection status for all AI providers:\n * - Ollama (local LLM)\n * - Lynkr (universal LLM proxy - local models from anywhere)\n * - Whisper (local STT)\n * - Cloud (OpenAI/Anthropic)\n */\n\nimport { NextRequest, NextResponse } from 'next/server';\nimport { checkOllamaHealth, type OllamaHealthStatus } from '@/lib/ollama';\nimport { checkLynkrHealth, type LynkrHealthStatus } from '@/lib/lynkr';\n\n// ============================================================================\n// Types\n// ============================================================================\n\ninterface ProviderStatus {\n  connected: boolean;\n  latencyMs?: number;\n  error?: string;\n  models?: string[];\n  version?: string;\n}\n\ninterface LynkrStatus extends ProviderStatus {\n  baseUrl: string;\n  tunnelUrl?: string;\n  provider?: string;\n  features?: {\n    memory?: boolean;\n    tools?: boolean;\n    streaming?: boolean;\n    embeddings?: boolean;\n  };\n}\n\ninterface HealthResponse {\n  timestamp: number;\n  providers: {\n    ollama: ProviderStatus & {\n      baseUrl: string;\n    };\n    lynkr: LynkrStatus;\n    whisper: ProviderStatus & {\n      enabled: boolean;\n    };\n    openai: ProviderStatus;\n    anthropic: ProviderStatus;\n  };\n  summary: {\n    localAvailable: boolean;\n    lynkrAvailable: boolean;\n    cloudAvailable: boolean;\n    recommendedProvider: 'local' | 'lynkr' | 'cloud';\n  };\n}\n\n// ============================================================================\n// Health Check Functions\n// ============================================================================\n\n/**\n * Check Ollama health\n */\nasync function checkOllama(baseUrl: string): Promise<OllamaHealthStatus> {\n  return checkOllamaHealth(baseUrl);\n}\n\n/**\n * Check Whisper local availability\n * TODO: Implement actual whisper check when local whisper is set up\n */\nasync function checkWhisper(): Promise<ProviderStatus & { enabled: boolean }> {\n  const whisperEnabled = process.env.WHISPER_LOCAL_ENABLED === 'true';\n\n  if (!whisperEnabled) {\n    return {\n      connected: false,\n      enabled: false,\n      error: 'Local Whisper not enabled',\n    };\n  }\n\n  // TODO: Implement actual whisper health check\n  // For now, return enabled status\n  return {\n    connected: whisperEnabled,\n    enabled: whisperEnabled,\n    latencyMs: 0,\n  };\n}\n\n/**\n * Check OpenAI API health\n */\nasync function checkOpenAI(): Promise<ProviderStatus> {\n  const apiKey = process.env.OPENAI_API_KEY;\n\n  if (!apiKey) {\n    return {\n      connected: false,\n      error: 'OPENAI_API_KEY not configured',\n    };\n  }\n\n  const start = Date.now();\n\n  try {\n    // Quick models list to verify API key works\n    const response = await fetch('https://api.openai.com/v1/models', {\n      method: 'GET',\n      headers: {\n        Authorization: `Bearer ${apiKey}`,\n      },\n      signal: AbortSignal.timeout(5000),\n    });\n\n    if (!response.ok) {\n      return {\n        connected: false,\n        error: `API returned ${response.status}`,\n        latencyMs: Date.now() - start,\n      };\n    }\n\n    const data = await response.json();\n    const models = data.data\n      ?.filter((m: { id: string }) => m.id.startsWith('gpt-'))\n      ?.map((m: { id: string }) => m.id)\n      ?.slice(0, 10) || [];\n\n    return {\n      connected: true,\n      latencyMs: Date.now() - start,\n      models,\n    };\n  } catch (error) {\n    return {\n      connected: false,\n      error: error instanceof Error ? error.message : 'Connection failed',\n      latencyMs: Date.now() - start,\n    };\n  }\n}\n\n/**\n * Check Anthropic API health\n */\nasync function checkAnthropic(): Promise<ProviderStatus> {\n  const apiKey = process.env.ANTHROPIC_API_KEY;\n\n  if (!apiKey) {\n    return {\n      connected: false,\n      error: 'ANTHROPIC_API_KEY not configured',\n    };\n  }\n\n  const start = Date.now();\n\n  try {\n    // Simple request to verify API key\n    // Anthropic doesn't have a /models endpoint, so we do a minimal completion\n    const response = await fetch('https://api.anthropic.com/v1/messages', {\n      method: 'POST',\n      headers: {\n        'x-api-key': apiKey,\n        'anthropic-version': '2023-06-01',\n        'Content-Type': 'application/json',\n      },\n      body: JSON.stringify({\n        model: 'claude-3-haiku-20240307',\n        max_tokens: 1,\n        messages: [{ role: 'user', content: 'hi' }],\n      }),\n      signal: AbortSignal.timeout(5000),\n    });\n\n    // Even if we get rate limited (429), the key is valid\n    if (response.ok || response.status === 429) {\n      return {\n        connected: true,\n        latencyMs: Date.now() - start,\n        models: ['claude-3-opus', 'claude-3-sonnet', 'claude-3-haiku'],\n      };\n    }\n\n    return {\n      connected: false,\n      error: `API returned ${response.status}`,\n      latencyMs: Date.now() - start,\n    };\n  } catch (error) {\n    return {\n      connected: false,\n      error: error instanceof Error ? error.message : 'Connection failed',\n      latencyMs: Date.now() - start,\n    };\n  }\n}\n\n/**\n * Check Lynkr proxy health\n * Tries tunnel URL first (for production/remote access), then local URL\n */\nasync function checkLynkr(baseUrl: string, tunnelUrl?: string): Promise<LynkrStatus> {\n  // Try tunnel URL first if available (for remote access)\n  const urlsToTry = tunnelUrl ? [tunnelUrl, baseUrl] : [baseUrl];\n\n  for (const url of urlsToTry) {\n    const status = await checkLynkrHealth(url);\n\n    if (status.connected) {\n      return {\n        baseUrl,\n        tunnelUrl: tunnelUrl,\n        connected: true,\n        latencyMs: status.latencyMs,\n        provider: status.provider,\n        version: status.version,\n        features: status.features,\n      };\n    }\n  }\n\n  // All URLs failed\n  const lastStatus = await checkLynkrHealth(urlsToTry[urlsToTry.length - 1]);\n  return {\n    baseUrl,\n    tunnelUrl,\n    connected: false,\n    latencyMs: lastStatus.latencyMs,\n    error: lastStatus.error ?? 'Lynkr not reachable',\n  };\n}\n\n// ============================================================================\n// Route Handler\n// ============================================================================\n\nexport async function GET(request: NextRequest) {\n  const { searchParams } = new URL(request.url);\n  const ollamaUrl = searchParams.get('ollamaUrl') ?? process.env.OLLAMA_BASE_URL ?? 'http://localhost:11434';\n  const lynkrUrl = searchParams.get('lynkrUrl') ?? process.env.LYNKR_BASE_URL ?? 'http://localhost:8081';\n  const lynkrTunnelUrl = searchParams.get('lynkrTunnelUrl') ?? process.env.LYNKR_TUNNEL_URL;\n  const checkCloud = searchParams.get('checkCloud') !== 'false';\n  const checkLynkrFlag = searchParams.get('checkLynkr') !== 'false';\n\n  // Run health checks in parallel\n  const [ollamaStatus, lynkrStatus, whisperStatus, openaiStatus, anthropicStatus] = await Promise.all([\n    checkOllama(ollamaUrl),\n    checkLynkrFlag ? checkLynkr(lynkrUrl, lynkrTunnelUrl) : Promise.resolve({\n      baseUrl: lynkrUrl,\n      connected: false,\n      error: 'Skipped',\n    } as LynkrStatus),\n    checkWhisper(),\n    checkCloud ? checkOpenAI() : Promise.resolve({ connected: false, error: 'Skipped' } as ProviderStatus),\n    checkCloud ? checkAnthropic() : Promise.resolve({ connected: false, error: 'Skipped' } as ProviderStatus),\n  ]);\n\n  // Determine availability\n  const localAvailable = ollamaStatus.connected;\n  const lynkrAvailable = lynkrStatus.connected;\n  const cloudAvailable = openaiStatus.connected || anthropicStatus.connected;\n\n  // Recommend provider based on availability and latency\n  // Priority: Lynkr (if available) > Local Ollama > Cloud\n  let recommendedProvider: 'local' | 'lynkr' | 'cloud' = 'cloud';\n\n  if (lynkrAvailable) {\n    // Lynkr is available - use it for \"anywhere\" access\n    recommendedProvider = 'lynkr';\n  } else if (localAvailable && (!cloudAvailable || (ollamaStatus.latencyMs ?? 0) < 1000)) {\n    // Local Ollama is available and fast\n    recommendedProvider = 'local';\n  }\n\n  const response: HealthResponse = {\n    timestamp: Date.now(),\n    providers: {\n      ollama: {\n        baseUrl: ollamaUrl,\n        connected: ollamaStatus.connected,\n        latencyMs: ollamaStatus.latencyMs,\n        error: ollamaStatus.error,\n        models: ollamaStatus.models.map(m => m.name),\n        version: ollamaStatus.version,\n      },\n      lynkr: lynkrStatus,\n      whisper: whisperStatus,\n      openai: openaiStatus,\n      anthropic: anthropicStatus,\n    },\n    summary: {\n      localAvailable,\n      lynkrAvailable,\n      cloudAvailable,\n      recommendedProvider,\n    },\n  };\n\n  return NextResponse.json(response);\n}\n"],"names":[],"mappings":";;;;AAAA;;;;;;;;;;CAUC,GAED;AACA;AAAA;AACA;AAAA;;;;AA+CA,+EAA+E;AAC/E,yBAAyB;AACzB,+EAA+E;AAE/E;;CAEC,GACD,eAAe,YAAY,OAAe;IACxC,OAAO,IAAA,qJAAiB,EAAC;AAC3B;AAEA;;;CAGC,GACD,eAAe;IACb,MAAM,iBAAiB,QAAQ,GAAG,CAAC,qBAAqB,KAAK;IAE7D,IAAI,CAAC,gBAAgB;QACnB,OAAO;YACL,WAAW;YACX,SAAS;YACT,OAAO;QACT;IACF;IAEA,8CAA8C;IAC9C,iCAAiC;IACjC,OAAO;QACL,WAAW;QACX,SAAS;QACT,WAAW;IACb;AACF;AAEA;;CAEC,GACD,eAAe;IACb,MAAM,SAAS,QAAQ,GAAG,CAAC,cAAc;IAEzC,IAAI,CAAC,QAAQ;QACX,OAAO;YACL,WAAW;YACX,OAAO;QACT;IACF;IAEA,MAAM,QAAQ,KAAK,GAAG;IAEtB,IAAI;QACF,4CAA4C;QAC5C,MAAM,WAAW,MAAM,MAAM,oCAAoC;YAC/D,QAAQ;YACR,SAAS;gBACP,eAAe,CAAC,OAAO,EAAE,QAAQ;YACnC;YACA,QAAQ,YAAY,OAAO,CAAC;QAC9B;QAEA,IAAI,CAAC,SAAS,EAAE,EAAE;YAChB,OAAO;gBACL,WAAW;gBACX,OAAO,CAAC,aAAa,EAAE,SAAS,MAAM,EAAE;gBACxC,WAAW,KAAK,GAAG,KAAK;YAC1B;QACF;QAEA,MAAM,OAAO,MAAM,SAAS,IAAI;QAChC,MAAM,SAAS,KAAK,IAAI,EACpB,OAAO,CAAC,IAAsB,EAAE,EAAE,CAAC,UAAU,CAAC,UAC9C,IAAI,CAAC,IAAsB,EAAE,EAAE,GAC/B,MAAM,GAAG,OAAO,EAAE;QAEtB,OAAO;YACL,WAAW;YACX,WAAW,KAAK,GAAG,KAAK;YACxB;QACF;IACF,EAAE,OAAO,OAAO;QACd,OAAO;YACL,WAAW;YACX,OAAO,iBAAiB,QAAQ,MAAM,OAAO,GAAG;YAChD,WAAW,KAAK,GAAG,KAAK;QAC1B;IACF;AACF;AAEA;;CAEC,GACD,eAAe;IACb,MAAM,SAAS,QAAQ,GAAG,CAAC,iBAAiB;IAE5C,IAAI,CAAC,QAAQ;QACX,OAAO;YACL,WAAW;YACX,OAAO;QACT;IACF;IAEA,MAAM,QAAQ,KAAK,GAAG;IAEtB,IAAI;QACF,mCAAmC;QACnC,2EAA2E;QAC3E,MAAM,WAAW,MAAM,MAAM,yCAAyC;YACpE,QAAQ;YACR,SAAS;gBACP,aAAa;gBACb,qBAAqB;gBACrB,gBAAgB;YAClB;YACA,MAAM,KAAK,SAAS,CAAC;gBACnB,OAAO;gBACP,YAAY;gBACZ,UAAU;oBAAC;wBAAE,MAAM;wBAAQ,SAAS;oBAAK;iBAAE;YAC7C;YACA,QAAQ,YAAY,OAAO,CAAC;QAC9B;QAEA,sDAAsD;QACtD,IAAI,SAAS,EAAE,IAAI,SAAS,MAAM,KAAK,KAAK;YAC1C,OAAO;gBACL,WAAW;gBACX,WAAW,KAAK,GAAG,KAAK;gBACxB,QAAQ;oBAAC;oBAAiB;oBAAmB;iBAAiB;YAChE;QACF;QAEA,OAAO;YACL,WAAW;YACX,OAAO,CAAC,aAAa,EAAE,SAAS,MAAM,EAAE;YACxC,WAAW,KAAK,GAAG,KAAK;QAC1B;IACF,EAAE,OAAO,OAAO;QACd,OAAO;YACL,WAAW;YACX,OAAO,iBAAiB,QAAQ,MAAM,OAAO,GAAG;YAChD,WAAW,KAAK,GAAG,KAAK;QAC1B;IACF;AACF;AAEA;;;CAGC,GACD,eAAe,WAAW,OAAe,EAAE,SAAkB;IAC3D,wDAAwD;IACxD,MAAM,YAAY,YAAY;QAAC;QAAW;KAAQ,GAAG;QAAC;KAAQ;IAE9D,KAAK,MAAM,OAAO,UAAW;QAC3B,MAAM,SAAS,MAAM,IAAA,mJAAgB,EAAC;QAEtC,IAAI,OAAO,SAAS,EAAE;YACpB,OAAO;gBACL;gBACA,WAAW;gBACX,WAAW;gBACX,WAAW,OAAO,SAAS;gBAC3B,UAAU,OAAO,QAAQ;gBACzB,SAAS,OAAO,OAAO;gBACvB,UAAU,OAAO,QAAQ;YAC3B;QACF;IACF;IAEA,kBAAkB;IAClB,MAAM,aAAa,MAAM,IAAA,mJAAgB,EAAC,SAAS,CAAC,UAAU,MAAM,GAAG,EAAE;IACzE,OAAO;QACL;QACA;QACA,WAAW;QACX,WAAW,WAAW,SAAS;QAC/B,OAAO,WAAW,KAAK,IAAI;IAC7B;AACF;AAMO,eAAe,IAAI,OAAoB;IAC5C,MAAM,EAAE,YAAY,EAAE,GAAG,IAAI,IAAI,QAAQ,GAAG;IAC5C,MAAM,YAAY,aAAa,GAAG,CAAC,gBAAgB,QAAQ,GAAG,CAAC,eAAe,IAAI;IAClF,MAAM,WAAW,aAAa,GAAG,CAAC,eAAe,QAAQ,GAAG,CAAC,cAAc,IAAI;IAC/E,MAAM,iBAAiB,aAAa,GAAG,CAAC,qBAAqB,QAAQ,GAAG,CAAC,gBAAgB;IACzF,MAAM,aAAa,aAAa,GAAG,CAAC,kBAAkB;IACtD,MAAM,iBAAiB,aAAa,GAAG,CAAC,kBAAkB;IAE1D,gCAAgC;IAChC,MAAM,CAAC,cAAc,aAAa,eAAe,cAAc,gBAAgB,GAAG,MAAM,QAAQ,GAAG,CAAC;QAClG,YAAY;QACZ,iBAAiB,WAAW,UAAU,kBAAkB,QAAQ,OAAO,CAAC;YACtE,SAAS;YACT,WAAW;YACX,OAAO;QACT;QACA;QACA,aAAa,gBAAgB,QAAQ,OAAO,CAAC;YAAE,WAAW;YAAO,OAAO;QAAU;QAClF,aAAa,mBAAmB,QAAQ,OAAO,CAAC;YAAE,WAAW;YAAO,OAAO;QAAU;KACtF;IAED,yBAAyB;IACzB,MAAM,iBAAiB,aAAa,SAAS;IAC7C,MAAM,iBAAiB,YAAY,SAAS;IAC5C,MAAM,iBAAiB,aAAa,SAAS,IAAI,gBAAgB,SAAS;IAE1E,uDAAuD;IACvD,wDAAwD;IACxD,IAAI,sBAAmD;IAEvD,IAAI,gBAAgB;QAClB,oDAAoD;QACpD,sBAAsB;IACxB,OAAO,IAAI,kBAAkB,CAAC,CAAC,kBAAkB,CAAC,aAAa,SAAS,IAAI,CAAC,IAAI,IAAI,GAAG;QACtF,qCAAqC;QACrC,sBAAsB;IACxB;IAEA,MAAM,WAA2B;QAC/B,WAAW,KAAK,GAAG;QACnB,WAAW;YACT,QAAQ;gBACN,SAAS;gBACT,WAAW,aAAa,SAAS;gBACjC,WAAW,aAAa,SAAS;gBACjC,OAAO,aAAa,KAAK;gBACzB,QAAQ,aAAa,MAAM,CAAC,GAAG,CAAC,CAAA,IAAK,EAAE,IAAI;gBAC3C,SAAS,aAAa,OAAO;YAC/B;YACA,OAAO;YACP,SAAS;YACT,QAAQ;YACR,WAAW;QACb;QACA,SAAS;YACP;YACA;YACA;YACA;QACF;IACF;IAEA,OAAO,yVAAY,CAAC,IAAI,CAAC;AAC3B"}}]
}